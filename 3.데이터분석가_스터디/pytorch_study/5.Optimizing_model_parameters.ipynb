{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83849f34-f971-4b0a-a8d7-8984cd11d603",
   "metadata": {},
   "source": [
    "## Optimizing model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd2a93a-04c5-4e67-8c82-db84f88ca75e",
   "metadata": {},
   "source": [
    "- 이제 모델과 데이터를 확보했으니 데이터에 대한 모델을 최적화하여 교육, 검증 및 테스트할 차례입니다. 모델을 훈련시키는 것은 반복적인 과정이다. 각 반복(에포크라고 함)에서 모델은 출력에 대해 추측하고, 추측(손실)의 오류를 계산하고, (이전 섹션에서 살펴본 바와 같이) 해당 매개변수에 대한 오차의 파생물을 수집하고, 기울기 강하를 사용하여 이러한 매개 변수를 최적화한다.m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680df3fe-d2f3-47a8-b4e6-32b8f72b7208",
   "metadata": {},
   "source": [
    "#### Prerequisite Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "950ab603-6d1b-42e1-b9ec-a28378bbc998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af496aea-6877-4327-8285-a4fe15b02f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/envs/pytorch_gpu/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1627336316785/work/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "185a8e24-63f8-490b-80ac-591a3fac84c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cafcb3a9-9c89-4269-9442-ca10a84088fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader=DataLoader(training_data, batch_size=64)\n",
    "test_dataloader=DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b0bce5e-e8c5-4c36-acdc-43ea480654ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.linear_relu_stack=nn.Sequential(\n",
    "            nn.Linear(28*28,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeeb676-1631-43f4-b05b-280372236cb8",
   "metadata": {},
   "source": [
    "#### Hyperparameters\n",
    "- 하이퍼 모수는 모형 최적화 프로세스를 제어할 수 있는 조정 가능한 모수입니다. 서로 다른 초 매개 변수 값은 모델 교육 및 수렴 속도에 영향을 미칠 수 있습니다(초 매개 변수 조정에 대한 자세한 내용 참조).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "603e98c2-e410-4bd4-9eda-de12f0f1bbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b729fa-1e9d-4318-8a7d-2277163c8e49",
   "metadata": {},
   "source": [
    "#### Optimization Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973ce268-95e7-4d22-9987-a238244f5706",
   "metadata": {},
   "source": [
    "- 하이퍼 파라미터를 설정하면 최적화 루프를 사용하여 모델을 교육하고 최적화할 수 있습니다. 최적화 루프의 각 반복을 epoch(신기원)이라고합니다.\n",
    "- Train Loop - 교육 데이터 세트에 대해 반복하고 최적의 매개 변수로 수렴하려고 합니다.\n",
    "- 검증/테스트 루프 - 테스트 데이터 세트에 대해 반복하여 모델 성능이 향상되고 있는지 확인합니다.\n",
    "- 이제 교육 루프에서 사용되는 몇 가지 개념을 간략히 숙지하겠습니다. 계속해서 최적화 루프의 전체 구현을 확인하십시오."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f8ab6f-e0f6-427c-97c3-db862e7765a2",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700966c6-7430-4098-88ec-82989da97717",
   "metadata": {},
   "source": [
    "- 일부 교육 데이터와 함께 제시될 때, 우리의 훈련되지 않은 네트워크는 정답을 제공하지 않을 가능성이 높다.\n",
    "- 손실함수는 얻은 결과와 목표값이 다른 정도를 측정하는데, 우리가 훈련 중에 최소화하고 싶은 것이 손실함수입니다.\n",
    "- 손실을 계산하기 위해 주어진 데이터 샘플의 입력을 사용하여 예측하고 실제 데이터 레이블 값과 비교한다.\n",
    "- Common loss functions include nn.MSELoss (Mean Square Error) for regression tasks, and nn.NLLLoss (Negative Log Likelihood) for classification. nn.CrossEntropyLoss combines nn.LogSoftmax and nn.NLLLoss.\n",
    "- 우리는 모델의 출력 로짓을 nn.CrossEntropyLoss에 전달하여 로짓을 정규화하고 예측 오차를 계산합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5905930-0cde-4e3d-9313-74ac81195687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d28f5e-95f0-401f-84fc-e013176efab4",
   "metadata": {},
   "source": [
    "#### Optimizer\n",
    "- Optimization is the process of adjusting model parameters to reduce model error in each training step. Optimization algorithms define how this process is performed (in this example we use Stochastic Gradient Descent). All optimization logic is encapsulated in the optimizer object. Here, we use the SGD optimizer; additionally, there are many different optimizers available in PyTorch such as ADAM and RMSProp, that work better for different kinds of models and data.\n",
    "\n",
    "We initialize the optimizer by registering the model’s parameters that need to be trained, and passing in the learning rate hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc9ff8b6-87d3-4636-9e5f-0f469f252b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a0fe14-7e76-40af-aa0b-1ef8c4847744",
   "metadata": {},
   "source": [
    "Inside the training loop, optimization happens in three steps:\n",
    "- Call optimizer.zero_grad() to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
    "- Backpropagate the prediction loss with a call to loss.backward(). PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
    "- Once we have our gradients, we call optimizer.step() to adjust the parameters by the gradients collected in the backward pass.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29051216-1e26-4d7d-86d3-e61d7edf538b",
   "metadata": {},
   "source": [
    "#### Full Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfa53d3-aee7-4ed6-80bb-8af9b060d3f0",
   "metadata": {},
   "source": [
    "- 우리는 최적화 코드를 통해 루프되는 train_loop과 테스트 데이터에 대해 모델의 성능을 평가하는 test_loop을 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1458ecf7-8ba7-4a4d-b44e-015b2bcc0768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2782e609-7259-452b-8ca5-a3aca3140faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        #Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        #Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbe0e8a-cc7d-4c00-8e59-df3122433077",
   "metadata": {},
   "source": [
    "- 손실 함수와 최적화 프로그램을 초기화하고 train_loop 및 test_loop에 전달합니다. 모델의 향상된 성능을 추적할 수 있는 epoch의 수를 얼마든지 늘릴 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8d2dfa8-c537-44e9-b972-3fb88f46b89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "----------------------------------\n",
      "loss: 2.300167 [    0/60000]\n",
      "loss: 2.289372 [ 6400/60000]\n",
      "loss: 2.266861 [12800/60000]\n",
      "loss: 2.255954 [19200/60000]\n",
      "loss: 2.247903 [25600/60000]\n",
      "loss: 2.222174 [32000/60000]\n",
      "loss: 2.220986 [38400/60000]\n",
      "loss: 2.188217 [44800/60000]\n",
      "loss: 2.186215 [51200/60000]\n",
      "loss: 2.153893 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Avg loss: 2.146412 \n",
      "\n",
      "Epoch 2\n",
      "----------------------------------\n",
      "loss: 2.159698 [    0/60000]\n",
      "loss: 2.150538 [ 6400/60000]\n",
      "loss: 2.086768 [12800/60000]\n",
      "loss: 2.098682 [19200/60000]\n",
      "loss: 2.052067 [25600/60000]\n",
      "loss: 1.992160 [32000/60000]\n",
      "loss: 2.011655 [38400/60000]\n",
      "loss: 1.928404 [44800/60000]\n",
      "loss: 1.936493 [51200/60000]\n",
      "loss: 1.861063 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.7%, Avg loss: 1.860088 \n",
      "\n",
      "Epoch 3\n",
      "----------------------------------\n",
      "loss: 1.897582 [    0/60000]\n",
      "loss: 1.871301 [ 6400/60000]\n",
      "loss: 1.744350 [12800/60000]\n",
      "loss: 1.782959 [19200/60000]\n",
      "loss: 1.684476 [25600/60000]\n",
      "loss: 1.631893 [32000/60000]\n",
      "loss: 1.646729 [38400/60000]\n",
      "loss: 1.551065 [44800/60000]\n",
      "loss: 1.585379 [51200/60000]\n",
      "loss: 1.474956 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 1.496767 \n",
      "\n",
      "Epoch 4\n",
      "----------------------------------\n",
      "loss: 1.567781 [    0/60000]\n",
      "loss: 1.536511 [ 6400/60000]\n",
      "loss: 1.383482 [12800/60000]\n",
      "loss: 1.456757 [19200/60000]\n",
      "loss: 1.353112 [25600/60000]\n",
      "loss: 1.341054 [32000/60000]\n",
      "loss: 1.349650 [38400/60000]\n",
      "loss: 1.279429 [44800/60000]\n",
      "loss: 1.320592 [51200/60000]\n",
      "loss: 1.220714 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Avg loss: 1.246900 \n",
      "\n",
      "Epoch 5\n",
      "----------------------------------\n",
      "loss: 1.327033 [    0/60000]\n",
      "loss: 1.306764 [ 6400/60000]\n",
      "loss: 1.141861 [12800/60000]\n",
      "loss: 1.249904 [19200/60000]\n",
      "loss: 1.136433 [25600/60000]\n",
      "loss: 1.154184 [32000/60000]\n",
      "loss: 1.169507 [38400/60000]\n",
      "loss: 1.110784 [44800/60000]\n",
      "loss: 1.153031 [51200/60000]\n",
      "loss: 1.071813 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 1.090628 \n",
      "\n",
      "Epoch 6\n",
      "----------------------------------\n",
      "loss: 1.166483 [    0/60000]\n",
      "loss: 1.160784 [ 6400/60000]\n",
      "loss: 0.982061 [12800/60000]\n",
      "loss: 1.118400 [19200/60000]\n",
      "loss: 1.000054 [25600/60000]\n",
      "loss: 1.025974 [32000/60000]\n",
      "loss: 1.056839 [38400/60000]\n",
      "loss: 1.001738 [44800/60000]\n",
      "loss: 1.041384 [51200/60000]\n",
      "loss: 0.977508 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.7%, Avg loss: 0.987873 \n",
      "\n",
      "Epoch 7\n",
      "----------------------------------\n",
      "loss: 1.052836 [    0/60000]\n",
      "loss: 1.064001 [ 6400/60000]\n",
      "loss: 0.870929 [12800/60000]\n",
      "loss: 1.028874 [19200/60000]\n",
      "loss: 0.912513 [25600/60000]\n",
      "loss: 0.933542 [32000/60000]\n",
      "loss: 0.982442 [38400/60000]\n",
      "loss: 0.929424 [44800/60000]\n",
      "loss: 0.962341 [51200/60000]\n",
      "loss: 0.913854 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.916634 \n",
      "\n",
      "Epoch 8\n",
      "----------------------------------\n",
      "loss: 0.967789 [    0/60000]\n",
      "loss: 0.995458 [ 6400/60000]\n",
      "loss: 0.790401 [12800/60000]\n",
      "loss: 0.964290 [19200/60000]\n",
      "loss: 0.853675 [25600/60000]\n",
      "loss: 0.864551 [32000/60000]\n",
      "loss: 0.929423 [38400/60000]\n",
      "loss: 0.880121 [44800/60000]\n",
      "loss: 0.904123 [51200/60000]\n",
      "loss: 0.867493 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.864683 \n",
      "\n",
      "Epoch 9\n",
      "----------------------------------\n",
      "loss: 0.901500 [    0/60000]\n",
      "loss: 0.943616 [ 6400/60000]\n",
      "loss: 0.729547 [12800/60000]\n",
      "loss: 0.915745 [19200/60000]\n",
      "loss: 0.811673 [25600/60000]\n",
      "loss: 0.812350 [32000/60000]\n",
      "loss: 0.889173 [38400/60000]\n",
      "loss: 0.845382 [44800/60000]\n",
      "loss: 0.860324 [51200/60000]\n",
      "loss: 0.831826 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.825176 \n",
      "\n",
      "Epoch 10\n",
      "----------------------------------\n",
      "loss: 0.848261 [    0/60000]\n",
      "loss: 0.902649 [ 6400/60000]\n",
      "loss: 0.682219 [12800/60000]\n",
      "loss: 0.878294 [19200/60000]\n",
      "loss: 0.779926 [25600/60000]\n",
      "loss: 0.771894 [32000/60000]\n",
      "loss: 0.856990 [38400/60000]\n",
      "loss: 0.819902 [44800/60000]\n",
      "loss: 0.826479 [51200/60000]\n",
      "loss: 0.803116 [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.793892 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n----------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405beee6-cef3-4b01-af14-d2d5b1245c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "pytorch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
