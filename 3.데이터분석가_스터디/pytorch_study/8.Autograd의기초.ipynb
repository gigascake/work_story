{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2846a1ed-c34d-4b49-bc8c-529169961404",
   "metadata": {},
   "source": [
    "## Autograd의 기초"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cd9772-8425-409a-9cb2-aa2852a3753e",
   "metadata": {},
   "source": [
    "- PyTorch의 Autograd 기능은 PyTorch를 기계 학습 프로젝트 구축을 위해 유연하고 빠르게 만드는 것의 일부입니다. 복잡한 계산을 통해 여러 편도함수( 그라디언트 라고도 함)를 빠르고 쉽게 계산할 수 있습니다 . 이 작업은 역전파 기반 신경망 학습의 핵심입니다.\n",
    "- autograd의 힘은 런타임에 계산을 동적으로 추적한다는 사실에서 비롯됩니다 . 즉, 모델에 결정 분기 또는 런타임까지 길이를 알 수 없는 루프가 있는 경우 계산이 여전히 올바르게 추적되고 올바른 결과를 얻을 수 있습니다. 학습을 유도하는 그라디언트. 이는 모델이 Python으로 구축되었다는 사실과 결합되어 그라디언트 계산을 위해 보다 엄격하게 구조화된 모델의 정적 분석에 의존하는 프레임워크보다 훨씬 더 많은 유연성을 제공합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab680c9-6983-4bac-a29b-232aa4e9d460",
   "metadata": {},
   "source": [
    "### Autograd가 필요한 이유는 무엇입니까?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aa0c3c-23ec-4504-a22d-dae300b4b2ec",
   "metadata": {},
   "source": [
    "- 오토그라드는 다음과 같다. 그것은 모든 계산의 역사를 추적한다. PyTorch 모델의 모든 계산 텐서는 입력 텐서와 이를 만드는 데 사용된 함수의 이력을 가지고 있다.\n",
    "- 각각 텐서에 작용하도록 의도된 파이토치 함수가 자체 도함수를 계산하기 위한 내장 구현을 가지고 있다는 사실과 결합하면 학습에 필요한 로컬 도함수의 계산 속도를 크게 높일 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5e3f6a-96e1-4810-b5f6-c804dafbf1e8",
   "metadata": {},
   "source": [
    "### A Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "610905a4-ef4d-4362-8027-ffd0f591ed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker \n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539f499e-97f0-4fc0-a1c5-629e701cb863",
   "metadata": {},
   "source": [
    "-  required_grad=True 이 플래그를 설정하는 것은 이어지는 모든 계산에서 autograd가 해당 계산의 출력 텐서에 계산 기록을 축적한다는 것을 의미한다.\n",
    "- torch.linspace : 값이 처음부터 끝까지 균등하게 배치된 1차원 텐서를 작성합니다. 즉, 값은 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42bb9c14-dbff-4928-b8f2-834b21a91686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.2618, 0.5236, 0.7854, 1.0472, 1.3090, 1.5708, 1.8326, 2.0944,\n",
      "        2.3562, 2.6180, 2.8798, 3.1416, 3.4034, 3.6652, 3.9270, 4.1888, 4.4506,\n",
      "        4.7124, 4.9742, 5.2360, 5.4978, 5.7596, 6.0214, 6.2832],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc44c2b2-c7fb-4a52-b9de-588343707a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f966c433ad0>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = torch.sin(a)\n",
    "plt.plot(a.detach(), b.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13294816-16b7-43ea-bfa1-0098ed6206d0",
   "metadata": {},
   "source": [
    "- Tensor가 기록을 추적하는 것을 중단하게 하려면, .detach()를 호출하여 연산 기록으로부터 분리(detach)하여 이후 연산들이 추적되는 것을 방지할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f8f522-c66f-4f65-a0dc-7e44543881d0",
   "metadata": {},
   "source": [
    "- 이 grad_fn은 역전파 단계를 실행하고 기울기를 계산할 때, 이 텐서의 모든 입력에 대한 sin(x)의 미분(도함수)을 계산할 필요가 있다는 힌트를 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33ab5669-d15c-4eb1-8bbc-8948e299f971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0000e+00,  5.1764e-01,  1.0000e+00,  1.4142e+00,  1.7321e+00,\n",
      "         1.9319e+00,  2.0000e+00,  1.9319e+00,  1.7321e+00,  1.4142e+00,\n",
      "         1.0000e+00,  5.1764e-01, -1.7485e-07, -5.1764e-01, -1.0000e+00,\n",
      "        -1.4142e+00, -1.7321e+00, -1.9319e+00, -2.0000e+00, -1.9319e+00,\n",
      "        -1.7321e+00, -1.4142e+00, -1.0000e+00, -5.1764e-01,  3.4969e-07],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([ 1.0000e+00,  1.5176e+00,  2.0000e+00,  2.4142e+00,  2.7321e+00,\n",
      "         2.9319e+00,  3.0000e+00,  2.9319e+00,  2.7321e+00,  2.4142e+00,\n",
      "         2.0000e+00,  1.5176e+00,  1.0000e+00,  4.8236e-01, -3.5763e-07,\n",
      "        -4.1421e-01, -7.3205e-01, -9.3185e-01, -1.0000e+00, -9.3185e-01,\n",
      "        -7.3205e-01, -4.1421e-01,  4.7684e-07,  4.8236e-01,  1.0000e+00],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "c = 2 * b\n",
    "print(c)\n",
    "\n",
    "d = c + 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ab837c-7ca4-44fa-a76c-6e78fa8baf37",
   "metadata": {},
   "source": [
    "- 마지막으로 단일 요소 출력을 계산해 보겠습니다. 인수가 없는 텐서에서 .backward()를 호출할 때, 손실 함수를 계산할 때처럼 호출 텐서는 단일 요소만 포함할 것으로 예상한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8912ae2-a7c1-4de3-ac5d-cfa1bd3dca4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = d.sum()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445d6d42-6599-4628-b2e0-b597903a50a3",
   "metadata": {},
   "source": [
    "- 우리의 텐서와 함께 저장된 각각의 grad_fn은 당신이 그것의 next_functions 속성으로 그것의 입력까지 연산을 할 수 있게 해준다. 아래에서 볼 수 있듯이 d에서 이 속성을 드릴다운하면 모든 선행 텐서에 대한 그라데이션 함수를 볼 수 있습니다. a.grad_fn은 없음으로 보고되며, 이는 자체 이력이 없는 함수에 대한 입력임을 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b54d6740-d30e-4b36-bfe6-14bd7b8f589c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\n",
      "<AddBackward0 object at 0x7f966c3466d0>\n",
      "((<MulBackward0 object at 0x7f966c346990>, 0), (None, 0))\n",
      "((<SinBackward object at 0x7f966c3466d0>, 0), (None, 0))\n",
      "((<AccumulateGrad object at 0x7f966c346b50>, 0),)\n",
      "()\n",
      "\n",
      "c:\n",
      "<MulBackward0 object at 0x7f966c346d90>\n",
      "\n",
      "b:\n",
      "<SinBackward object at 0x7f966c346d90>\n",
      "\n",
      "a:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('d:')\n",
    "print(d.grad_fn)\n",
    "print(d.grad_fn.next_functions)\n",
    "print(d.grad_fn.next_functions[0][0].next_functions)\n",
    "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print(d.grad_fn.next_functions[0][0].next_functions[0][0].next_functions[0][0].next_functions)\n",
    "print('\\nc:')\n",
    "print(c.grad_fn)\n",
    "print('\\nb:')\n",
    "print(b.grad_fn)\n",
    "print('\\na:')\n",
    "print(a.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae42e67b-7e56-44be-a62e-19a83f4cb2ae",
   "metadata": {},
   "source": [
    "- With all this machinery in place, how do we get derivatives out? You call the backward() method on the output, and check the input’s grad property to inspect the gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf112d5c-2bfa-46fb-9b68-b12a66d15ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.0000e+00,  1.9319e+00,  1.7321e+00,  1.4142e+00,  1.0000e+00,\n",
      "         5.1764e-01, -8.7423e-08, -5.1764e-01, -1.0000e+00, -1.4142e+00,\n",
      "        -1.7321e+00, -1.9319e+00, -2.0000e+00, -1.9319e+00, -1.7321e+00,\n",
      "        -1.4142e+00, -1.0000e+00, -5.1764e-01,  2.3850e-08,  5.1764e-01,\n",
      "         1.0000e+00,  1.4142e+00,  1.7321e+00,  1.9319e+00,  2.0000e+00])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f96634060d0>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "out.backward()\n",
    "print(a.grad)\n",
    "plt.plot(a.detach(), a.grad.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69103d6f-3f6a-4155-b6a0-2bbdff802ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
    "b = torch.sin(a)\n",
    "c = 2 * b\n",
    "d = c + 1\n",
    "out = d.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c80063f6-dabc-4f3e-8ec1-592383d2fc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71dc67b-2e28-469b-8325-48d32a57466c",
   "metadata": {},
   "source": [
    "- 계산의 리프 노드만 그레이디언트를 계산한다는 점에 유의하십시오. 예를 들어 인쇄(c.grad)를 시도하면 없음이 반환됩니다. 이 간단한 예에서는 입력만 리프 노드이므로, 입력만 그레이디언트가 계산된다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c0cae8-ce04-43dd-8227-75f04b3bb752",
   "metadata": {},
   "source": [
    "### Autograd in Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec4d884-babb-47bd-b0c8-fd5aa8af40ea",
   "metadata": {},
   "source": [
    "- 오토그라드가 어떻게 작동하는지 간략히 살펴봤지만, 목적대로 사용되었을 때 어떻게 보일까요? 작은 모델을 정의하고 단일 교육 배치 후 어떻게 변화하는지 알아보겠습니다. 먼저 몇 가지 상수, 모델 및 입력 및 출력에 대한 몇 가지 대역을 정의합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "934f410f-351f-4159-8b0c-9c106213da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "DIM_IN = 1000\n",
    "HIDDEN_SIZE = 100\n",
    "DIM_OUT = 10\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TinyModel, self).__init__()\n",
    "\n",
    "        self.layer1 = torch.nn.Linear(1000, 100)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.layer2 = torch.nn.Linear(100, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "some_input = torch.randn(BATCH_SIZE, DIM_IN, requires_grad=False)\n",
    "ideal_output = torch.randn(BATCH_SIZE, DIM_OUT, requires_grad=False)\n",
    "\n",
    "model = TinyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dded4eb-4ea9-49b9-9cf5-74cdbc2210a5",
   "metadata": {},
   "source": [
    "- One thing you might notice is that we never specify requires_grad=True for the model’s layers. Within a subclass of torch.nn.Module, it’s assumed that we want to track gradients on the layers’ weights for learning.\n",
    "- 모델의 레이어를 살펴보면 가중치의 값을 검사하고 아직 그라디언트가 계산되지 않았 음을 확인할 수 있습니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4476d403-fdc5-4f00-afba-9fadf942c244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0981,  0.0572, -0.0513, -0.0210, -0.0861, -0.0190,  0.0889,  0.0679,\n",
      "         0.0308, -0.0888], grad_fn=<SliceBackward>)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.layer2.weight[0][0:10]) # just a small slice\n",
    "print(model.layer2.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a6705-d8ab-4f3a-adc1-6f6da7b13db5",
   "metadata": {},
   "source": [
    "- 하나의 교육 배치를 실행해 볼 때 이것이 어떻게 변하는지 알아보겠습니다. 손실 함수에 대해서는 예측과 이상_출력 사이의 유클리드 거리의 제곱을 사용하고 기본적인 확률적 경사 강하 최적화 도구를 사용할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cb3dce8-df88-47fc-a967-84b9b3e0962c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(150.4544, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "prediction = model(some_input)\n",
    "\n",
    "loss = (ideal_output - prediction).pow(2).sum()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57e9baed-6cf9-46bc-87ae-e0de16bdd341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0981,  0.0572, -0.0513, -0.0210, -0.0861, -0.0190,  0.0889,  0.0679,\n",
      "         0.0308, -0.0888], grad_fn=<SliceBackward>)\n",
      "tensor([ 4.2519, -2.7065,  0.2412, -2.9373,  0.0599,  3.8087,  3.0970,  0.5012,\n",
      "        -0.8951,  4.3931])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(model.layer2.weight[0][0:10])\n",
    "print(model.layer2.weight.grad[0][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7fd736-4e4c-4e60-9df9-1b81becfc959",
   "metadata": {},
   "source": [
    "- 각 학습 가중치에 대해 기울기가 계산되었지만 최적화 프로그램을 아직 실행하지 않았기 때문에 가중치는 변경되지 않았습니다. 최적화 도구는 계산된 그라데이션에 따라 모델 가중치를 업데이트하는 역할을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dbbb259-be3e-4070-9bfb-a85d36b810a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.2519, -2.7065,  0.2412, -2.9373,  0.0599,  3.8087,  3.0970,  0.5012,\n",
      "        -0.8951,  4.3931])\n",
      "tensor([ 25.5115, -16.2392,   1.4469, -17.6239,   0.3594,  22.8524,  18.5821,\n",
      "          3.0074,  -5.3706,  26.3588])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(model.layer2.weight.grad[0][0:10])\n",
    "\n",
    "for i in range(0, 5):\n",
    "    prediction = model(some_input)\n",
    "    loss = (ideal_output - prediction).pow(2).sum()\n",
    "    loss.backward()\n",
    "\n",
    "print(model.layer2.weight.grad[0][0:10])\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "print(model.layer2.weight.grad[0][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9054394-cc8e-438b-822d-32c5010d2b3c",
   "metadata": {},
   "source": [
    "- 위의 셀을 실행한 후, loss.backward()를 여러 번 실행하면 대부분의 그라데이션의 크기가 훨씬 커집니다. 다음 교육 배치를 실행하기 전에 그레이디언트를 0으로 설정하지 않으면 그레이디언트가 이러한 방식으로 폭발하여 부정확하고 예측 불가능한 학습 결과가 초래됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70c51d5-4539-4efa-a819-82b9a790f5fb",
   "metadata": {},
   "source": [
    "### Turning Autograd Off and On"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac073d84-8769-48dc-bd3b-dbe146ea110e",
   "metadata": {},
   "source": [
    "- autograd 활성화 여부를 세부적으로 제어해야 하는 경우가 있습니다. 이렇게 하는 방법은 상황에 따라 여러 가지가 있습니다. 가장 간단한 방법은 텐서 상의 required_grad 플래그를 직접 변경하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c3069cb-e28f-418a-8eaa-1e0eb96d6d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], requires_grad=True)\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]], grad_fn=<MulBackward0>)\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 3, requires_grad=True)\n",
    "print(a)\n",
    "\n",
    "b1 = 2 * a\n",
    "print(b1)\n",
    "\n",
    "a.requires_grad = False\n",
    "b2 = 2 * a\n",
    "print(b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d11fb4-6ae3-49df-a023-5ed6573fc393",
   "metadata": {},
   "source": [
    "- 위의 셀에서, 우리는 b1이 grad_fn (즉, 추적된 계산 이력)을 갖는 것을 볼 수 있는데, 이것은 오토그라드가 켜진 텐서 a로부터 파생되었기 때문에 우리가 예상하는 것이다. a.requires_grad = False로 명시적으로 autograd를 끄면, b2를 계산할 때 볼 수 있듯이 계산 내역이 더 이상 추적되지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7272a18-ec89-47e4-9976-a372a5c50dc6",
   "metadata": {},
   "source": [
    "- autograd를 일시적으로만 해제해야 하는 경우 torch.no_gradp를 사용하는 것이 더 나은 방법입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6655f56b-1451-4d87-9a60-e30570b9b756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]], grad_fn=<AddBackward0>)\n",
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]])\n",
      "tensor([[6., 6., 6.],\n",
      "        [6., 6., 6.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 3, requires_grad=True) * 2\n",
    "b = torch.ones(2, 3, requires_grad=True) * 3\n",
    "\n",
    "c1 = a + b\n",
    "print(c1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    c2 = a + b\n",
    "\n",
    "print(c2)\n",
    "\n",
    "c3 = a * b\n",
    "print(c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2abea6e-d790-4bad-af2f-2a0340e129d4",
   "metadata": {},
   "source": [
    "- torch.no_grad() can also be used as a function or method dectorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03954fdb-d372-40e9-af4b-30bfbba43f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]], grad_fn=<AddBackward0>)\n",
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "def add_tensors1(x, y):\n",
    "    return x + y\n",
    "\n",
    "@torch.no_grad()\n",
    "def add_tensors2(x, y):\n",
    "    return x + y\n",
    "\n",
    "\n",
    "a = torch.ones(2, 3, requires_grad=True) * 2\n",
    "b = torch.ones(2, 3, requires_grad=True) * 3\n",
    "\n",
    "c1 = add_tensors1(a, b)\n",
    "print(c1)\n",
    "\n",
    "c2 = add_tensors2(a, b)\n",
    "print(c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b5e40a-39b3-4f07-a704-ec4ef502f83f",
   "metadata": {},
   "source": [
    "- There’s a corresponding context manager, torch.enable_grad(), for turning autograd on when it isn’t already. It may also be used as a decorator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42695c8a-5458-46a6-8007-bbc9a283a008",
   "metadata": {},
   "source": [
    "- 마지막으로, 그라데이션 추적이 필요한 텐서가 있을 수 있지만 그렇지 않은 복사본을 원할 수 있습니다. 이를 위해 텐서 객체의 분리() 방법이 있다 - 계산 이력에서 분리된 텐서의 사본을 만든다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f00bf6c-e3f9-46bb-a460-8fb0ce8836ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0129, 0.2872, 0.9309, 0.0081, 0.0394], requires_grad=True)\n",
      "tensor([0.0129, 0.2872, 0.9309, 0.0081, 0.0394])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, requires_grad=True)\n",
    "y = x.detach()\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aeca2f-e930-4213-b61a-47ac0b4b99ac",
   "metadata": {},
   "source": [
    "- 위에서 텐서 몇 개를 그래프로 표시하려고 했습니다. 이는 matplotlib이 입력으로 NumPy 배열을 기대하며, required_grad=True인 텐서에 대해서는 PyTorch 텐서에서 NumPy 배열로의 암시적 변환이 활성화되지 않기 때문이다. 분리된 사본을 만드는 것은 우리가 앞으로 나아갈 수 있게 해줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ae522b-16b3-41ba-8ef3-75c85d8a5e76",
   "metadata": {},
   "source": [
    "### Autograd and In-place Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7502d304-964d-4970-9877-70aaf255039c",
   "metadata": {},
   "source": [
    "-  지금까지 이 노트북의 모든 예에서 변수를 사용하여 계산의 중간 값을 캡처했습니다. 오토그라드는 기울기 계산을 수행하기 위해 이러한 중간 값이 필요하다. 따라서 autograd를 사용할 때 인플레이스 작업을 사용할 때 주의해야 합니다.\n",
    "- 그렇게 하면 역방향 호출에서 도함수를 계산하는 데 필요한 정보가 파괴될 수 있습니다. PyTorch는 당신이 아래와 같이 autograd를 필요로 하는 leaf 변수에서 in-place 연산을 시도해도 당신을 정지시킬 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9557bbf-7c17-4abb-aa09-fc4ccd61b802",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_50776/3270092781.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "a = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n",
    "torch.sin_(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222e4873-c95d-4e71-8d3f-e39439113025",
   "metadata": {},
   "source": [
    "### Autograd Profiler\n",
    "- https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8500230-285d-4986-9f56-5d0371c2be94",
   "metadata": {},
   "source": [
    "- Autograd는 계산의 모든 단계를 자세히 추적합니다. 이러한 계산 히스토리와 타이밍 정보가 결합되면 유용한 프로파일러가 될 수 있으며, 오토그라드에는 그러한 기능이 내장되어 있다. 다음은 간단한 사용 예입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b13b0fc-c15b-48ce-9865-ce73ad0ae25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "           Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "---------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "      aten::div        50.52%      13.693ms        52.92%      14.344ms      14.344us      13.906ms        53.05%      13.906ms      13.906us          1000  \n",
      "      aten::mul        44.73%      12.124ms        47.08%      12.762ms      12.762us      12.307ms        46.95%      12.307ms      12.307us          1000  \n",
      "    aten::empty         4.75%       1.288ms         4.75%       1.288ms       0.644us       0.000us         0.00%       0.000us       0.000us          2000  \n",
      "---------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 27.105ms\n",
      "Self CUDA time total: 26.213ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "run_on_gpu = False\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:1')\n",
    "    run_on_gpu = True\n",
    "    \n",
    "x = torch.randn(2, 3, requires_grad=True)\n",
    "y = torch.rand(2, 3, requires_grad=True)\n",
    "z = torch.ones(2, 3, requires_grad=True)\n",
    "\n",
    "with torch.autograd.profiler.profile(use_cuda=run_on_gpu) as prf:\n",
    "    for _ in range(1000):\n",
    "        z = ( z / x ) * y\n",
    "\n",
    "print(prf.key_averages().table(sort_by='self_cpu_time_total'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f011014-5a5c-4ff0-84ac-14dd5325baae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "           Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "---------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "      aten::mul        47.76%      13.486ms        50.30%      14.203ms      14.203us      13.834ms        50.26%      13.834ms      13.834us          1000  \n",
      "      aten::div        47.09%      13.297ms        49.70%      14.035ms      14.035us      13.692ms        49.74%      13.692ms      13.692us          1000  \n",
      "    aten::empty         5.15%       1.455ms         5.15%       1.455ms       0.728us       0.000us         0.00%       0.000us       0.000us          2000  \n",
      "---------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 28.238ms\n",
      "Self CUDA time total: 27.526ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "run_on_gpu = False\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    run_on_gpu = True\n",
    "    \n",
    "x = torch.randn(2, 3, requires_grad=True)\n",
    "y = torch.rand(2, 3, requires_grad=True)\n",
    "z = torch.ones(2, 3, requires_grad=True)\n",
    "\n",
    "with torch.autograd.profiler.profile(use_cuda=run_on_gpu) as prf:\n",
    "    for _ in range(1000):\n",
    "        z = ( z / x ) * y\n",
    "\n",
    "print(prf.key_averages().table(sort_by='self_cpu_time_total'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dab9cd6-de56-4cc3-b0d0-1ab7e1e06d9c",
   "metadata": {},
   "source": [
    "### Advanced Topic: More Autograd Detail and the High-Level API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cbc54f-d788-46d5-a8b6-5c585d2e577a",
   "metadata": {},
   "source": [
    "- If you have a function with an n-dimensional input and m-dimensional output, \\vec{y}=f(\\vec{x}) \n",
    "y\n",
    "​\n",
    " =f( \n",
    "x\n",
    " ), the complete gradient is a matrix of the derivative of every output with respect to every input, called the Jacobian:\n",
    "- n차원 입력과 m차원 출력을 갖는 함수를 갖는다면, {vec{y}=f({vec}x}) y =f( x ), 완전한 그라디언트는 Jacobian이라고 불리는 모든 입력에 대한 모든 출력의 미분 행렬입니다.\n",
    "- 좀 더 구체적으로, 첫 번째 함수를 파이토치 모델(잠재적으로 많은 입력과 출력을 가진)로, 두 번째 함수를 손실 함수(모델의 출력을 입력으로, 손실 값을 스칼라 출력으로)로 가정하자.\n",
    "- 이러한 이유로, 백워드() 호출은 선택적 벡터 입력을 취할 수도 있다. 이 벡터는 텐서 위의 그레이디언트 집합을 나타내며, 그 앞에 오는 오토그라드 추적 텐서의 야코비안으로 곱한다. 작은 벡터로 구체적인 예를 들어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7df5568-be80-400b-b014-e59e604d4a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-970.0334,  418.4893,  -44.0985], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0f9044-6e6b-429c-a44c-8d184de75ec1",
   "metadata": {},
   "source": [
    "- 지금 y.backward()를 호출하면 런타임 오류가 발생하고 스칼라 출력에 대해서만 그라디언트를 암시 적으로 계산할 수 있다는 메시지가 표시됩니다. 다차원 출력의 경우, autograd는 Jacobian으로 곱할 수 있는 세 가지 출력에 대해 그라디언트를 제공할 것으로 기대합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11ea4e98-1a3e-4cbe-bbeb-9f4a920126db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float) # stand-in for gradients\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba5a87d-4c7e-4a8e-bc5d-15bf6315b22e",
   "metadata": {},
   "source": [
    "### The High-Level API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8297508-9048-43d5-86b4-df2c1e699855",
   "metadata": {},
   "source": [
    "- 중요한 미분 행렬 및 벡터 연산에 직접 액세스할 수 있는 API가 autograd에 있습니다. 특히 특정 입력에 대한 특정 함수의 야코비 행렬과 헤시안 행렬을 계산할 수 있습니다. (헤시안은 야코비안과 유사하지만, 모든 부분적인 이차 도함수를 표현한다.) 또한 이러한 행렬과 함께 벡터 곱을 취하는 방법을 제공합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b36e17-c41c-4fe0-adfd-d927d9c01688",
   "metadata": {},
   "source": [
    "- 2개의 단일 원소 입력에 대해 평가되는 단순 함수의 야코비안을 구해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08de0d8e-52e6-4fb9-8213-0275417ce41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.2675]), tensor([0.6201]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[2.6133]]), tensor([[3.]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exp_adder(x, y):\n",
    "    return 2 * x.exp() + 3 * y\n",
    "\n",
    "inputs = (torch.rand(1), torch.rand(1)) # arguments for the function\n",
    "print(inputs)\n",
    "torch.autograd.functional.jacobian(exp_adder, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fc282a3-85ae-4bf3-bfa9-07c035233243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.2389, 0.3015, 0.0572]), tensor([0.9067, 0.7062, 0.0331]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[2.5398, 0.0000, 0.0000],\n",
       "         [0.0000, 2.7039, 0.0000],\n",
       "         [0.0000, 0.0000, 2.1177]]),\n",
       " tensor([[3., 0., 0.],\n",
       "         [0., 3., 0.],\n",
       "         [0., 0., 3.]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = (torch.rand(3), torch.rand(3)) # arguments for the function\n",
    "print(inputs)\n",
    "torch.autograd.functional.jacobian(exp_adder, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7fac0c-8459-4549-8ab4-f1a9a888dadb",
   "metadata": {},
   "source": [
    "- The torch.autograd.functional.hessian() method works identically (assuming your function is twice differentiable), but returns a matrix of all second derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2504f6-0ee7-414a-af7b-a11dfd1fa3aa",
   "metadata": {},
   "source": [
    "- 벡터를 제공하면 벡터-야코비아 곱을 직접 계산하는 함수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "293fa803-a269-49a4-842e-7e004c245a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 869.1605, -945.6362, -572.6524]),\n",
       " tensor([4.0960e+02, 4.0960e+03, 4.0960e-01]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def do_some_doubling(x):\n",
    "    y = x * 2\n",
    "    while y.data.norm() < 1000:\n",
    "        y = y * 2\n",
    "    return y\n",
    "\n",
    "inputs = torch.randn(3)\n",
    "my_gradients = torch.tensor([0.1, 1.0, 0.0001])\n",
    "torch.autograd.functional.vjp(do_some_doubling, inputs, v=my_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc413aee-d032-4275-b4af-81ee5f59196f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "pytorch_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
